{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e448a-2aff-40fd-b721-b4f1baa07691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Setup\n",
    "# ! sudo apt install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender1 libxext6\n",
    "# ! pip install open-iris==1.0.0 faiss-cpu seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765202e0-2135-4dce-a4d2-fc0d3fe39f0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports, Constants, and Base Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b0cd0-fc4b-41de-a454-8f4d7b8c40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import BytesIO\n",
    "import pickle\n",
    "import iris\n",
    "import scipy\n",
    "import psutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import threading\n",
    "from itertools import combinations, product\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955219b-b483-41e5-9f18-8f87facafccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp, ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60449ef9-9f69-4792-b22a-ac0b7678b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 6 # Fit to CPU\n",
    "DIM = (2, 32, 200)\n",
    "X, Y = DIM [1:]\n",
    "MAX_ROT = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896019eb-f356-42dd-86f8-fefdd08cb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample_size = 5000\n",
    "gaussian_base_path = 'data/voter_param_search_gaussian_low_init/'\n",
    "uniform_base_path = 'data/voter_param_search_uniform_init/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970143a-2e75-4ac6-8408-48a5afdf40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boolean_iris(matrix, title=''):\n",
    "    plt.imshow(matrix, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20761b65-4529-4891-883e-6123ad8311d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(path, base_path=uniform_base_path, num_samples=single_sample_size, X=X//2, Y=Y):\n",
    "    return (\n",
    "        np.unpackbits(np.fromfile(base_path+path, dtype=np.uint8), bitorder=\"little\")\n",
    "        .reshape(num_samples, X, Y)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b569f45-be94-4021-b194-a80ad4a554ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Real Irises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73112ef-4223-4a37-a808-bf7c7ee9e2f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Raw Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462cd9c-e71e-450c-8474-a8cf707d9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (16, 200)\n",
    "# DEV = \"-dev\" # Access test data.\n",
    "DEV = \"\" # Access real data.\n",
    "print(\"Working on simulated data\" if DEV else \"Working on real data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f6176-8515-4617-b1cd-1686ffd82c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'wld-inversed-data-sharing' + DEV\n",
    "role_arn = 'arn:aws:iam::387760840988:role/worldcoin-data' + DEV\n",
    "metadata_path = 'metadata.csv'\n",
    "\n",
    "def memoize(func):\n",
    "    cache = {}\n",
    "    def memoized_func(*args):\n",
    "        if args in cache:\n",
    "            return cache[args]\n",
    "        result = func(*args)\n",
    "        cache[args] = result\n",
    "        return result\n",
    "    return memoized_func\n",
    "\n",
    "def assume_role(role_arn, session_name=\"S3ReadSession\"):\n",
    "    sts_client = boto3.client('sts')\n",
    "    assumed_role_object = sts_client.assume_role(\n",
    "        RoleArn=role_arn,\n",
    "        RoleSessionName=session_name\n",
    "    )\n",
    "    credentials = assumed_role_object['Credentials']\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=credentials['AccessKeyId'],\n",
    "        aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "        aws_session_token=credentials['SessionToken']\n",
    "    )\n",
    "    return s3\n",
    "\n",
    "# Assume the role and get credentials\n",
    "s3 = assume_role(role_arn, \"S3ReadSession\")\n",
    "\n",
    "def read_s3_file(bucket_name, file_key):\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    return BytesIO(obj['Body'].read())\n",
    "\n",
    "@memoize\n",
    "def load_response(image_id):\n",
    "    \" Return IrisFilterResponse \"\n",
    "    path = \"iris_filter_responses/\" + image_id + \".pickle\"\n",
    "    try:\n",
    "        pkl_file = read_s3_file(bucket_name, path)\n",
    "        return pickle.load(pkl_file)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return None\n",
    "\n",
    "@memoize\n",
    "def load_template(image_id):\n",
    "    \" Return IrisTemplate \"\n",
    "    path = \"iris_templates/\" + image_id + \".pickle\"\n",
    "    try:\n",
    "        pkl_file = read_s3_file(bucket_name, path)\n",
    "        return pickle.load(pkl_file)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return None\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "meta = pd.read_csv(read_s3_file(bucket_name, metadata_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10bc6e-39fc-4a2b-8668-341e91016981",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e15b5-61e9-410f-a8ea-e4074aff34e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102a01b-68c2-4884-8c93-393012b3c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for iterators.\n",
    "def take(count, it):\n",
    "    \" Take at most `count` items from the iterator `it` \"\n",
    "    for x in it:\n",
    "        if count is not None:\n",
    "            if count <= 0:\n",
    "                break\n",
    "            count -= 1\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836e9f8-8be1-4a4b-bff2-325ebcc1a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matching pairs.\n",
    "def iter_matching_image_ids(meta, unique_subjects):\n",
    "    \" Iterate matching pairs in the form (subject_id, ir_image_id_0, ir_image_id_1). \"\n",
    "    subject_ids = meta[\"subject_id\"].unique()\n",
    "\n",
    "    for side in [0, 1]:\n",
    "        meta_side = meta[meta[\"biological_side\"] == side]\n",
    "\n",
    "        for subject in subject_ids:\n",
    "            signups = meta_side[meta_side[\"subject_id\"] == subject]\n",
    "            if len(signups) < 2:\n",
    "                continue\n",
    "\n",
    "            L = 2 if unique_subjects else len(signups)\n",
    "\n",
    "            for i in range(L - 1):\n",
    "                for j in range(i + 1, L):\n",
    "                    yield (f\"{subject}_side{side}\", signups[\"ir_image_id\"].iloc[i], signups[\"ir_image_id\"].iloc[j])\n",
    "\n",
    "def load_matching_image_ids(meta, unique_subjects):\n",
    "    \" Return matching pairs in the form (subject_id, ir_image_id_0, ir_image_id_1), shuffled. \"    \n",
    "    pair_image_ids = list(iter_matching_image_ids(meta, unique_subjects))\n",
    "    rng = np.random.default_rng(seed=12345)\n",
    "    rng.shuffle(pair_image_ids)\n",
    "    return pair_image_ids\n",
    "\n",
    "def iter_related_pairs(meta, unique_subjects):\n",
    "    \" Iterate matching pairs in the form (subject_id, response_0, response_1). \"\n",
    "    for (subject, img_i, img_j) in load_matching_image_ids(meta, unique_subjects):\n",
    "        res_i = load_response(img_i)\n",
    "        res_j = load_response(img_j)\n",
    "        if res_i and res_j:\n",
    "            yield (subject, res_i, res_j)\n",
    "\n",
    "def load_related_pairs(meta, count=None, unique_subjects=False):\n",
    "    \" Return matching pairs in the form (subject_id, response_0, response_1). \"\n",
    "    return list(take(count, iter_related_pairs(meta, unique_subjects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f43732-bac3-4ca6-aaed-b51a8e4ce670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking methodologies\n",
    "def fill_masked_with_random(bits, mask):\n",
    "    filler = np.random.randint(0, 2, size=bits.shape, dtype=bool)\n",
    "    filler &= not_(mask)\n",
    "    bits ^= filler\n",
    "\n",
    "def fill_masked_with_zeros(bits, mask):\n",
    "    bits &= mask\n",
    "\n",
    "# Techniques that do not support masking will work, although with a modified scale of distances.\n",
    "# The change in distance can be calculated from the size of the overlap of masks. Alternatively,\n",
    "# it can be estimated with the expected average of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2afeb3-e259-4351-b54d-1565ec28e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make encoders from parameters.\n",
    "def make_encoder(v_subsample=1, h_subsample=1, top=True, bottom=True, real=True, imag=True, mask_threshold=0.9, static_mask=None, mask_with_random=False):\n",
    "\n",
    "    res_indexes = (top and [0] or []) + (bottom and [1] or [])\n",
    "    assert res_indexes, \"require top, bottom, or both\"\n",
    "\n",
    "    quantizers = (real and [np.real] or []) + (imag and [np.imag] or [])\n",
    "    assert quantizers, \"require real, imag, or both\"\n",
    "    \n",
    "    def encode(response):\n",
    "        bit_parts = []\n",
    "        mask_parts = []\n",
    "        \n",
    "        for res_index in res_indexes:\n",
    "            for quantizer in quantizers:\n",
    "                res = response.iris_responses[res_index][::v_subsample, ::h_subsample]\n",
    "                bits = quantizer(res) > 0\n",
    "                mask = response.mask_responses[res_index][::v_subsample, ::h_subsample] >= mask_threshold\n",
    "\n",
    "                if mask_with_random:\n",
    "                    # Replace masked bits with random bits.\n",
    "                    fill_masked_with_random(bits, mask)\n",
    "                \n",
    "                if static_mask is not None:\n",
    "                    # Remove the bits not selected by the static mask.\n",
    "                    fill_masked_with_zeros(bits, static_mask[::v_subsample, ::h_subsample])\n",
    "                    # Treat non-selected bits as masked (False).\n",
    "                    mask &= static_mask[::v_subsample, ::h_subsample]\n",
    "                \n",
    "                bit_parts.append(bits)\n",
    "                mask_parts.append(mask)\n",
    "                assert mask.shape == bits.shape\n",
    "\n",
    "        return np.concatenate(bit_parts), np.concatenate(mask_parts)\n",
    "    \n",
    "    return encode\n",
    "\n",
    "def encode_pairs(pairs, encode_fn):\n",
    "    return [\n",
    "        (subject_id, encode_fn(response_a), encode_fn(response_b))\n",
    "        for subject_id, response_a, response_b in pairs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2986ed9-5e2c-4b63-b902-0c92523cddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances\n",
    "def masked_distance(x, x_mask, y, y_mask):\n",
    "    mask = x_mask & y_mask\n",
    "    hd = np.sum((x ^ y) & mask)\n",
    "    return hd / np.sum(mask)\n",
    "\n",
    "def masked_rotate(x, rotation):\n",
    "    return (\n",
    "        np.roll(x[0], rotation, axis=1),\n",
    "        np.roll(x[1], rotation, axis=1),\n",
    "    )\n",
    "\n",
    "def distance(x, y):\n",
    "    return masked_distance(x[0], x[1], y[0], y[1])\n",
    "\n",
    "def distance_raw(raw_x, raw_y):\n",
    "    return distance(encode_high(raw_x), encode_high(raw_y))\n",
    "\n",
    "def rotate_raw(raw_x, rotation):\n",
    "    iris_responses = [\n",
    "        np.roll(r, rotation, axis=1)\n",
    "        for r in raw_x.iris_responses\n",
    "    ]\n",
    "    mask_responses = [\n",
    "        np.roll(r, rotation, axis=1)\n",
    "        for r in raw_x.mask_responses\n",
    "    ]\n",
    "    return iris.IrisFilterResponse(iris_responses=iris_responses, mask_responses=mask_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5342022-6bd3-4cca-ad65-7c760ef584c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotations.\n",
    "def without_rotation(pairs, distance_fn, rotate_fn, max_rotation):\n",
    "    for subject_id, x, y in pairs:\n",
    "        distances = [\n",
    "            distance_fn(x, rotate_fn(y, rotation))\n",
    "            for rotation in range(-max_rotation, max_rotation+1)\n",
    "        ]\n",
    "        best_rotation = -max_rotation + np.argmin(distances)        \n",
    "        y_aligned = rotate_fn(y, best_rotation)\n",
    "        yield (subject_id, x, y_aligned)\n",
    "\n",
    "def remove_rotation(pairs, distance_fn=distance, rotate_fn=masked_rotate, max_rotation=15):\n",
    "    return list(without_rotation(pairs, distance_fn, rotate_fn, max_rotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76d8fb-86ad-4405-8bb9-123be0c7dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boolean_iris(matrix, title=''):\n",
    "    plt.imshow(matrix, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02024c46-22e8-406a-bcc5-d25fa048fb6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282e0ee-578e-41a3-869f-0d433508c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 min\n",
    "encode_high = make_encoder()\n",
    "related_pairs = load_related_pairs(meta, count=None, unique_subjects=False)\n",
    "related_pairs_norot = remove_rotation(related_pairs, distance_fn=distance_raw, rotate_fn=rotate_raw)\n",
    "related_pairs_high = encode_pairs(related_pairs_norot, encode_high)\n",
    "shape_high = related_pairs_high[0][1][0].shape\n",
    "print(f\"Finished loading {len(related_pairs_high)} pairs,\", \"High-res\", shape_high, np.prod(shape_high), \"bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3c245-431e-41e5-9b12-30018dd2088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_array = np.array(related_pairs_high, dtype=object)\n",
    "subject_ids = np.repeat(tuples_array[:, 0], 2)  # Repeat each subject_id twice\n",
    "flattened_result = [item for tup in tuples_array for item in tup[1:]]\n",
    "iris_matrices, mask_matrices = zip(*flattened_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91deb9-4e4a-4f80-be97-b0386068e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_iris_df = pd.DataFrame({\n",
    "    'subject_id': subject_ids,\n",
    "    'iris_matrices': iris_matrices,\n",
    "    'mask_matrices': mask_matrices\n",
    "})\n",
    "true_iris_df['side'] = true_iris_df['subject_id'].apply(lambda x: x[-1])\n",
    "true_iris_df['subject_id'] = true_iris_df['subject_id'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67897c5c-d9a4-40aa-b23f-cbd82deabf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "true_iris_df['iris_matrices_bytes'] = true_iris_df['iris_matrices'].apply(lambda matrix: matrix.tobytes())\n",
    "true_iris_df['mask_matrices_bytes'] = true_iris_df['mask_matrices'].apply(lambda matrix: matrix.tobytes())\n",
    "true_iris_df = (\n",
    "    true_iris_df\n",
    "    .drop_duplicates(subset=['subject_id', 'iris_matrices_bytes', 'mask_matrices_bytes'])\n",
    "    .drop(columns=['iris_matrices_bytes', 'mask_matrices_bytes'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(f'Final iris DataFrame contains {len(true_iris_df)} different samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42c9b4-d049-4b52-bf61-bd5de0da0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_true_iris_df = true_iris_df.drop_duplicates(subset=['subject_id', 'side'])\n",
    "print(f'Unique iris DataFrame contains {len(unique_true_iris_df)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79695af3-6b77-4772-9dd1-9455faa18aec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Synthetic Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250d520-2479-49f0-96cf-6f8799bda7b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rotation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597b27b-39b5-4918-85aa-a13577cd7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "MAX_ROT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0e52e-062b-4215-9acb-5067b42d8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_iris_rotations_distance_dict = dict()\n",
    "for rot in range(-MAX_ROT, MAX_ROT+1):\n",
    "    if rot == 0:\n",
    "        continue\n",
    "    low_rotations, high_rotations = zip(\n",
    "        *true_iris_df['iris_matrices'].apply(\n",
    "            lambda matrix: [np.sum(part != np.roll(part, shift=rot, axis=1)) / part.size for part in np.split(matrix, 2, axis=0)]\n",
    "        )\n",
    "    )\n",
    "    real_iris_rotations_distance_dict[rot] = high_rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79231b-fab7-4ea8-a4f1-8eb4f78d8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for base_path in [uniform_base_path]: # [gaussian_base_path, uniform_base_path]\n",
    "    base_legend = pd.read_csv(base_path+'manifest.csv')\n",
    "    for index, (filename, num_samples, iterations, bias, init_method) in base_legend.iterrows():\n",
    "        data = import_data(path=filename, base_path=base_path, num_samples=num_samples)\n",
    "        for rot in range(-MAX_ROT, MAX_ROT+1):\n",
    "            if rot == 0:\n",
    "                continue\n",
    "            rolled_data = np.roll(data, shift=rot, axis=2)\n",
    "            rotated_distances = (np.sum(data != rolled_data, axis=(1, 2)) / ((X//2) * Y))\n",
    "            \n",
    "            ks_stat, ks_p_value = ks_2samp(rotated_distances, real_iris_rotations_distance_dict[rot]) # Kolmogorov-Smirnov test\n",
    "            t_stat, t_p_value = ttest_ind(rotated_distances, real_iris_rotations_distance_dict[rot]) # Student's t test\n",
    "                \n",
    "            results.append({\n",
    "                'Rotation':rot,\n",
    "                'Source':init_method.replace('_',' ')+f', {iterations} iterations, {bias} bias',\n",
    "                'ks_stat':ks_stat,\n",
    "                't_stat':t_stat,\n",
    "                'ks_p_value':ks_p_value,\n",
    "                't_p_value':t_p_value,\n",
    "                'mean':rotated_distances.mean(),\n",
    "                'std':rotated_distances.std(),\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['passed_KS_test'] = results_df['ks_p_value'] >= alpha\n",
    "results_df['passed_t_test'] = results_df['t_p_value'] >= alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8bbb4-4fe6-48b8-9102-8d97ea95a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_tests_results_df = (\n",
    "    results_df\n",
    "    .groupby('Source')[['ks_p_value', 'passed_KS_test', 't_p_value', 'passed_t_test']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(['ks_p_value', 't_p_value'], ascending=False)\n",
    ")\n",
    "statistical_tests_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92b49f-4afe-4f33-943b-4cf3ac1ba0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_reach_peak = results_df.groupby('Source')['mean'].max() > 0.51\n",
    "any_reach_peak[any_reach_peak].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdfd867-f069-45ed-beec-e8bd8063c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_tests_results_df = (\n",
    "    results_df[results_df['Rotation'] <= 6] # Before the \"peak\"\n",
    "    .groupby('Source')[['ks_p_value', 'passed_KS_test', 't_p_value', 'passed_t_test']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values(['ks_p_value', 't_p_value'], ascending=False)\n",
    ")\n",
    "statistical_tests_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d8a9d-8f8e-456d-a6c3-b9edd7b05b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_irises_rotations_df = pd.DataFrame(\n",
    "    [\n",
    "        pd.Series(\n",
    "            {\n",
    "                'mean':np.mean(real_iris_rotations_distance_dict[rot]),\n",
    "                'std':np.std(real_iris_rotations_distance_dict[rot]),\n",
    "                'Source':'real irises',\n",
    "                'Rotation':rot\n",
    "            }\n",
    "        ) for rot in range(-MAX_ROT, MAX_ROT+1) if rot != 0\n",
    "    ]\n",
    ").melt(\n",
    "    id_vars=['Rotation', 'Source'],\n",
    "    value_vars=['mean', 'std'],\n",
    "    var_name='Metric',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fff53-10b7-4b3a-ba48-96870d899413",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_sources = statistical_tests_results_df.head()['Source'].tolist()\n",
    "plot_df = pd.melt(\n",
    "    results_df[results_df['Source'].isin(potential_sources)],\n",
    "    id_vars=['Rotation', 'Source'],\n",
    "    value_vars=['mean', 'std'],\n",
    "    var_name='Metric',\n",
    ")\n",
    "plot_df = pd.concat([plot_df, real_irises_rotations_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dd14e-590f-424f-acab-ea3f846fe3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in potential_sources:\n",
    "    mask = plot_df['Source'].isin(['real irises']+[source])\n",
    "    facetgrid = sns.FacetGrid(plot_df[mask], col='Metric', height=5, aspect=2, sharex=False, sharey=False)\n",
    "    facetgrid.map_dataframe(sns.lineplot, x='Rotation', y='value', hue='Source', palette='husl')\n",
    "    [(ax.grid(True), ax.legend()) for ax in facetgrid.axes.flat]\n",
    "    facetgrid.fig.suptitle(f\"Mean and Std of real and {source.replace('_', ' ')} iris samples, in relation to rotation\", fontsize=15, y=1.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697cbfb-db16-43a5-80ef-c57cbde2c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ROT = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b54241e-3962-48c6-bc56-b80fedac1af3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Boolean Ratio Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fe8d4-3d98-4dfd-b3e8-e6e732b4bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df_lst = []\n",
    "for base_path in [uniform_base_path]: # [gaussian_base_path, uniform_base_path]\n",
    "    base_legend = pd.read_csv(base_path+'manifest.csv')\n",
    "    for index, (filename, num_samples, iterations, bias, init_method) in base_legend.iterrows():\n",
    "        data = import_data(path=filename, base_path=base_path, num_samples=num_samples)\n",
    "        ratio_df_lst.append(\n",
    "            pd.DataFrame(\n",
    "                pd.Series(data.mean(axis=(1,2))).rename('Boolean Ratio')\n",
    "            ).assign(\n",
    "                Source = init_method.replace('_',' '),\n",
    "                Iterations = iterations,\n",
    "                Bias = bias,\n",
    "            )\n",
    "        )\n",
    "ratio_df = pd.concat(ratio_df_lst).melt(id_vars=['Boolean Ratio', 'Source'], value_vars=['Iterations', 'Bias'], var_name='Feature')\n",
    "ratio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251e31c-a828-42b6-85b5-622e3b5f4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetgrid = sns.FacetGrid(ratio_df, col='Feature', hue='Source', palette='husl', sharex=False, sharey=False, height=4, aspect=1.8)\n",
    "facetgrid.map_dataframe(sns.lineplot, x='value', y='Boolean Ratio')\n",
    "facetgrid.fig.suptitle(f\"True / False Ratio Validation\", fontsize=20, y=1.2)\n",
    "facetgrid.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a98c5-c0e7-4df7-a77d-1bda025d97d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Nearest to Random Dist Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ff6d5-f674-42d4-8901-a125c0753377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_rotated_matrices(matrices, max_rotation):\n",
    "    return np.vstack([\n",
    "        np.roll(matrix, shift, axis=0).flatten()\n",
    "        for matrix in matrices\n",
    "        for shift in range(-max_rotation, max_rotation + 1)\n",
    "    ])\n",
    "\n",
    "def get_min_and_random_dist_across_rotations(iris_matrices, mask_matrices, max_rotation):\n",
    "    num_matrices = len(iris_matrices)\n",
    "    num_rotations = 2 * max_rotation + 1\n",
    "\n",
    "    # Rotate matrices and masks, reshape to (num_matrices, num_rotations, flattened_size)\n",
    "    rotated_matrices = stack_rotated_matrices(iris_matrices, max_rotation).reshape(num_matrices, num_rotations, -1)\n",
    "    rotated_masks = stack_rotated_matrices(mask_matrices, max_rotation).reshape(num_matrices, num_rotations, -1)\n",
    "\n",
    "    closest_distances, random_distances = [], []\n",
    "    for i in range(num_matrices):\n",
    "        # Current matrix rotations and masks\n",
    "        current_rotated_matrix = rotated_matrices[i]\n",
    "        current_rotated_mask = rotated_masks[i]\n",
    "\n",
    "        # Extract other matrices' rotations excluding the current\n",
    "        other_rotated_matrices = np.delete(rotated_matrices, i, axis=0).reshape(-1, rotated_matrices.shape[-1])\n",
    "        other_rotated_masks = np.delete(rotated_masks, i, axis=0).reshape(-1, rotated_masks.shape[-1])\n",
    "\n",
    "        # Calculate valid positions and Hamming distances\n",
    "        valid_positions = current_rotated_mask[:, None] & other_rotated_masks\n",
    "        differences = current_rotated_matrix[:, None] != other_rotated_matrices\n",
    "\n",
    "        # Calculate Hamming distances\n",
    "        hamming_distances = np.sum(differences & valid_positions, axis=-1) / np.sum(valid_positions, axis=-1)\n",
    "        \n",
    "        # Find the minimum distance and a random distance\n",
    "        closest_distances.append(np.min(hamming_distances))\n",
    "        random_distances.append(np.random.choice(hamming_distances.flatten()))\n",
    "\n",
    "    return pd.DataFrame({\"closest_dist\": closest_distances, \"random_dist\": random_distances})\n",
    "\n",
    "def load_and_reshape_masks(filename, num_masks):\n",
    "    flattened_data = np.unpackbits(np.fromfile(filename, dtype=np.uint8))\n",
    "    boolean_arrays = flattened_data.reshape((num_masks, X//2, Y))\n",
    "    vertically_stacked = np.tile(boolean_arrays, (1, 2, 1))\n",
    "    # duplicated_arrays = np.repeat(vertically_stacked[:, np.newaxis, :, :], DIM[0], axis=1) # For full irises \n",
    "    return vertically_stacked\n",
    "\n",
    "def int_to_scaled_string(n):\n",
    "    suffixes = ['', 'K', 'M', 'B', 'T']\n",
    "    idx = max(0, min(len(suffixes) - 1, int((len(str(abs(n))) - 1) / 3)))\n",
    "    scaled = n / (1000 ** idx)\n",
    "    return f\"{scaled:.1f}{suffixes[idx]}\" if scaled % 1 else f\"{int(scaled)}{suffixes[idx]}\"\n",
    "\n",
    "def process_file(filename, num_samples, iterations, bias, init_method, base_path, num_unique_samples, loaded_masks):\n",
    "    data = import_data(path=filename, base_path=base_path, num_samples=num_samples)\n",
    "    random_indices = np.random.choice(num_samples, size=2*num_unique_samples, replace=False)\n",
    "    sampled_data = np.take(data, random_indices, axis=0).reshape(num_unique_samples, X, Y)\n",
    "    random_indices = np.random.choice(num_samples, size=num_unique_samples, replace=False)\n",
    "    sampled_masks = np.take(loaded_masks, random_indices, axis=0)\n",
    "    return (\n",
    "        get_min_and_random_dist_across_rotations(sampled_data, sampled_masks, max_rotation=MAX_ROT)\n",
    "        .assign(Source=init_method.replace('_', ' ') + f', {iterations} iterations, {bias} bias')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae130a45-d0e5-4532-9b23-b627771d3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_wavelets, high_wavelets = np.split(np.stack(unique_true_iris_df['iris_matrices'].values), 2, axis=1)\n",
    "low_wavelets_masks, high_wavelets_masks = np.split(np.stack(unique_true_iris_df['mask_matrices'].values), 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55029f99-e3c4-4da0-a1e0-9e70eaf85156",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_irises_dist = get_min_and_random_dist_across_rotations(high_wavelets, high_wavelets_masks, max_rotation=MAX_ROT).assign(Source = 'Real Irises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db76a8-4031-4e53-a7d6-fb79b024ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_masks = 1000000\n",
    "path = f'data/synthetic_iris_data/{int_to_scaled_string(num_masks)}_mask_arrays.dat'\n",
    "loaded_masks = load_and_reshape_masks(path, num_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b7681-aad5-48d3-88d4-4f2f1c083808",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for base_path in [uniform_base_path]: # [gaussian_base_path, uniform_base_path]\n",
    "    base_legend = pd.read_csv(base_path + 'manifest.csv')\n",
    "    parallel_results = Parallel(n_jobs=n_jobs)(delayed(process_file)(\n",
    "        filename, single_sample_size, iterations, bias, init_method, base_path, len(unique_true_iris_df), loaded_masks\n",
    "    ) for index, (filename, num_samples, iterations, bias, init_method) in base_legend.iterrows())\n",
    "    results.extend(parallel_results)\n",
    "results_df = pd.concat([real_irises_dist]+[x for x in results if x is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cb21b-c398-4731-9272-9c3d59ec3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = results_df.groupby('Source')['closest_dist'].mean().sort_values().index[:6]\n",
    "plot_df = pd.melt(\n",
    "    results_df[results_df['Source'].isin(best_results)], \n",
    "    id_vars='Source', \n",
    "    value_vars=['random_dist', 'closest_dist'],\n",
    "    var_name='Distance From',\n",
    "    value_name='Hamming Distance'\n",
    ")\n",
    "plot_df['Distance From'] = plot_df['Distance From'].apply(lambda x: x.split('_')[0].capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ecb1e-e945-46d9-879c-cd78a80d28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetgrid = sns.FacetGrid(plot_df, col='Distance From', hue='Source', palette='husl', height=4, aspect=2, sharex=False, sharey=False)\n",
    "facetgrid.map_dataframe(sns.histplot, x='Hamming Distance', stat='probability', kde=True)\n",
    "[ax.grid(True) for ax in facetgrid.axes.flat]\n",
    "facetgrid.add_legend()\n",
    "facetgrid.fig.suptitle(f\"Distance from Random / Nearest iris (only low wavelets)\\nVarious Data Sources\", fontsize=20, y=1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32222ce9-4477-4738-8374-54dade3d2ff0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Comparing pair-wise distance distributions (to Daugman survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7addff89-f77d-4aba-9046-0d3a8e3f9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance_wrapper(filename, num_samples, iterations, bias, init_method, base_path, num_real_irises=len(unique_true_iris_df)):\n",
    "    data = import_data(path=filename, base_path=base_path, num_samples=num_samples)\n",
    "    random_indices = np.random.choice(num_samples, size=num_real_irises, replace=False)\n",
    "    distances = pdist(np.take(data, random_indices, axis=0).reshape(num_real_irises, -1), metric='hamming')\n",
    "    return pd.DataFrame({\n",
    "        'Distances':distances,\n",
    "        'Source':init_method.replace('_', ' ') + f', {iterations} iterations, {bias} bias',\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888dd8ec-3851-4fb5-b158-aa8bd33cf2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_wavelets, high_wavelets = np.split(np.stack(unique_true_iris_df['iris_matrices'].values), 2, axis=1)\n",
    "real_iris_matrices = high_wavelets.reshape(len(unique_true_iris_df), -1)\n",
    "real_pairwise_distances = pdist(real_iris_matrices, metric='hamming')\n",
    "real_iris_df = pd.DataFrame({\n",
    "    'Distances':real_pairwise_distances,\n",
    "    'Source':'Real Irises',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77732add-a183-4ac4-848b-996b2b83ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_results = []\n",
    "for base_path in [uniform_base_path]: # [gaussian_base_path, uniform_base_path]\n",
    "    base_legend = pd.read_csv(base_path + 'manifest.csv')\n",
    "    parallel_results = Parallel(n_jobs=n_jobs)(delayed(pairwise_distance_wrapper)(\n",
    "        filename, single_sample_size, iterations, bias, init_method, base_path\n",
    "    ) for index, (filename, num_samples, iterations, bias, init_method) in base_legend.iterrows())\n",
    "    pairwise_results.extend(parallel_results)\n",
    "pairwise_results_df = pd.concat([real_iris_df]+[x for x in pairwise_results if x is not None])\n",
    "pairwise_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4195c5-05a0-458e-8940-6990d7737097",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_sources = abs(pairwise_results_df.groupby('Source')['Distances'].mean() - pairwise_results_df.loc[pairwise_results_df['Source'] == 'Real Irises', 'Distances'].mean()).sort_values()\n",
    "potential_sources = potential_sources[:6].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262fb85-fede-41ad-bf10-4e7c23ea2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.histplot(pairwise_results_df[pairwise_results_df['Source'].isin(potential_sources)], x='Distances', stat='probability', hue='Source', palette='husl', kde=True)\n",
    "plt.grid(True)\n",
    "plt.title('Pairwise Distance Distribution', fontsize=15, y=1.07)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeee330-4576-43d1-a261-e9e5ff69b159",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visually view several samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb9c99-fd41-4ac7-81c4-0dcbf58a3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_legend = pd.read_csv(uniform_base_path + 'manifest.csv')\n",
    "filename = '5k_voter_arrays_14k_b010_un.dat'\n",
    "data = import_data(path=filename)\n",
    "random_indices = np.random.choice(single_sample_size, size=4, replace=False)\n",
    "for i in random_indices:\n",
    "    plot_boolean_iris(np.vstack((data[i], data[i+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c19cb-20be-4fe8-b9ac-6ccd57a551d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b9b06-9bcd-4c55-b87a-249c96fd0129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
