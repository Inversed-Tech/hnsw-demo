{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e448a-2aff-40fd-b721-b4f1baa07691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Setup\n",
    "# ! sudo apt install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxrender1 libxext6\n",
    "# ! pip install open-iris==1.0.0 faiss-cpu seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765202e0-2135-4dce-a4d2-fc0d3fe39f0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0a4b5-71ff-4b99-8b85-f20134c9fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from io import BytesIO\n",
    "from itertools import combinations, product\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e3b1c-861f-454a-8c2b-30e1fcac7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from scipy.stats import ks_2samp, ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c645f-001c-46a7-82ed-48b5e28b771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 6 # Fit to CPU\n",
    "DIM = (2, 32, 200)\n",
    "X, Y = DIM [1:]\n",
    "MAX_ROT = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b569f45-be94-4021-b194-a80ad4a554ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73112ef-4223-4a37-a808-bf7c7ee9e2f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Real Irises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462cd9c-e71e-450c-8474-a8cf707d9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (16, 200)\n",
    "# DEV = \"-dev\" # Access test data.\n",
    "DEV = \"\" # Access real data.\n",
    "print(\"Working on simulated data\" if DEV else \"Working on real data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0f6176-8515-4617-b1cd-1686ffd82c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'wld-inversed-data-sharing' + DEV\n",
    "role_arn = 'arn:aws:iam::387760840988:role/worldcoin-data' + DEV\n",
    "metadata_path = 'metadata.csv'\n",
    "\n",
    "def memoize(func):\n",
    "    cache = {}\n",
    "    def memoized_func(*args):\n",
    "        if args in cache:\n",
    "            return cache[args]\n",
    "        result = func(*args)\n",
    "        cache[args] = result\n",
    "        return result\n",
    "    return memoized_func\n",
    "\n",
    "def assume_role(role_arn, session_name=\"S3ReadSession\"):\n",
    "    sts_client = boto3.client('sts')\n",
    "    assumed_role_object = sts_client.assume_role(\n",
    "        RoleArn=role_arn,\n",
    "        RoleSessionName=session_name\n",
    "    )\n",
    "    credentials = assumed_role_object['Credentials']\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=credentials['AccessKeyId'],\n",
    "        aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "        aws_session_token=credentials['SessionToken']\n",
    "    )\n",
    "    return s3\n",
    "\n",
    "# Assume the role and get credentials\n",
    "s3 = assume_role(role_arn, \"S3ReadSession\")\n",
    "\n",
    "def read_s3_file(bucket_name, file_key):\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    return BytesIO(obj['Body'].read())\n",
    "\n",
    "@memoize\n",
    "def load_response(image_id):\n",
    "    \" Return IrisFilterResponse \"\n",
    "    path = \"iris_filter_responses/\" + image_id + \".pickle\"\n",
    "    try:\n",
    "        pkl_file = read_s3_file(bucket_name, path)\n",
    "        return pickle.load(pkl_file)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return None\n",
    "\n",
    "@memoize\n",
    "def load_template(image_id):\n",
    "    \" Return IrisTemplate \"\n",
    "    path = \"iris_templates/\" + image_id + \".pickle\"\n",
    "    try:\n",
    "        pkl_file = read_s3_file(bucket_name, path)\n",
    "        return pickle.load(pkl_file)\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return None\n",
    "\n",
    "# Read the file into a DataFrame\n",
    "meta = pd.read_csv(read_s3_file(bucket_name, metadata_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc3653-a323-4e67-8194-8851206b6e7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Synthetic Irises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909cff73-5e46-44de-9646-6bb81d9aae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_iris(method, num_samples, path='compressed_iris_matrices'):\n",
    "    # Load data and randomly select num_samples samples\n",
    "    loaded_data = np.load(f'{path}_{method}.npz')['data']\n",
    "    assert loaded_data.shape[0] >= num_samples, f\"Requested {num_samples} samples, but only {loaded_data.shape[0]} available.\"\n",
    "    indices = np.random.choice(loaded_data.shape[0], num_samples, replace=False)\n",
    "    return loaded_data[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fbc5a-ce07-40b0-8b88-dbe37df54ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict()\n",
    "for method in ['gaussian', 'voter', 'voter_gaussian']:\n",
    "    data_dict[method] = load_synthetic_iris(method, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20761b65-4529-4891-883e-6123ad8311d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_voter_model_rust_implementation(path, total_num_samples, num_samples=None, DIM=DIM):\n",
    "    num_samples = num_samples if num_samples else total_num_samples\n",
    "    assert num_samples <= total_num_samples\n",
    "    data = np.fromfile(path, dtype=np.uint8)\n",
    "    return (\n",
    "        np.unpackbits(data, bitorder=\"little\")\n",
    "        .reshape(total_num_samples, *DIM[1:])\n",
    "        [np.random.choice(total_num_samples, size=num_samples, replace=False)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ac4d4-0637-4706-ab96-f88e7a216d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bryan_low_data = import_voter_model_rust_implementation('2M_voter_arrays_80k_b45.dat', 1000000, 1000)\n",
    "bryan_high_data = import_voter_model_rust_implementation('2M_voter_arrays_7k_b13.dat', 1000000, 1000)\n",
    "data_dict['voter_bryan'] = np.concatenate([bryan_low_data, bryan_high_data], axis=1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547eb39-37d5-4f32-a86f-06d665df1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df = pd.concat(\n",
    "    [pd.DataFrame({'iris_matrices':list(data), 'source':source}) for source, data in data_dict.items()], \n",
    "    ignore_index=True\n",
    ")\n",
    "synthetic_df['mask_matrices'] = [np.ones((reduce(mul, DIM[:2]), DIM[-1])).astype(bool)] * len(synthetic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc09308-9166-49fd-b18a-2c027e8ef553",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10bc6e-39fc-4a2b-8668-341e91016981",
   "metadata": {},
   "source": [
    "## Real Irises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501e15b5-61e9-410f-a8ea-e4074aff34e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102a01b-68c2-4884-8c93-393012b3c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for iterators.\n",
    "def take(count, it):\n",
    "    \" Take at most `count` items from the iterator `it` \"\n",
    "    for x in it:\n",
    "        if count is not None:\n",
    "            if count <= 0:\n",
    "                break\n",
    "            count -= 1\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836e9f8-8be1-4a4b-bff2-325ebcc1a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load matching pairs.\n",
    "def iter_matching_image_ids(meta, unique_subjects):\n",
    "    \" Iterate matching pairs in the form (subject_id, ir_image_id_0, ir_image_id_1). \"\n",
    "    subject_ids = meta[\"subject_id\"].unique()\n",
    "\n",
    "    for side in [0, 1]:\n",
    "        meta_side = meta[meta[\"biological_side\"] == side]\n",
    "\n",
    "        for subject in subject_ids:\n",
    "            signups = meta_side[meta_side[\"subject_id\"] == subject]\n",
    "            if len(signups) < 2:\n",
    "                continue\n",
    "\n",
    "            L = 2 if unique_subjects else len(signups)\n",
    "\n",
    "            for i in range(L - 1):\n",
    "                for j in range(i + 1, L):\n",
    "                    yield (f\"{subject}_side{side}\", signups[\"ir_image_id\"].iloc[i], signups[\"ir_image_id\"].iloc[j])\n",
    "\n",
    "def load_matching_image_ids(meta, unique_subjects):\n",
    "    \" Return matching pairs in the form (subject_id, ir_image_id_0, ir_image_id_1), shuffled. \"    \n",
    "    pair_image_ids = list(iter_matching_image_ids(meta, unique_subjects))\n",
    "    rng = np.random.default_rng(seed=12345)\n",
    "    rng.shuffle(pair_image_ids)\n",
    "    return pair_image_ids\n",
    "\n",
    "def iter_related_pairs(meta, unique_subjects):\n",
    "    \" Iterate matching pairs in the form (subject_id, response_0, response_1). \"\n",
    "    for (subject, img_i, img_j) in load_matching_image_ids(meta, unique_subjects):\n",
    "        res_i = load_response(img_i)\n",
    "        res_j = load_response(img_j)\n",
    "        if res_i and res_j:\n",
    "            yield (subject, res_i, res_j)\n",
    "\n",
    "def load_related_pairs(meta, count=None, unique_subjects=False):\n",
    "    \" Return matching pairs in the form (subject_id, response_0, response_1). \"\n",
    "    return list(take(count, iter_related_pairs(meta, unique_subjects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f43732-bac3-4ca6-aaed-b51a8e4ce670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking methodologies\n",
    "def fill_masked_with_random(bits, mask):\n",
    "    filler = np.random.randint(0, 2, size=bits.shape, dtype=bool)\n",
    "    filler &= not_(mask)\n",
    "    bits ^= filler\n",
    "\n",
    "def fill_masked_with_zeros(bits, mask):\n",
    "    bits &= mask\n",
    "\n",
    "# Techniques that do not support masking will work, although with a modified scale of distances.\n",
    "# The change in distance can be calculated from the size of the overlap of masks. Alternatively,\n",
    "# it can be estimated with the expected average of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2afeb3-e259-4351-b54d-1565ec28e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make encoders from parameters.\n",
    "def make_encoder(v_subsample=1, h_subsample=1, top=True, bottom=True, real=True, imag=True, mask_threshold=0.9, static_mask=None, mask_with_random=False):\n",
    "\n",
    "    res_indexes = (top and [0] or []) + (bottom and [1] or [])\n",
    "    assert res_indexes, \"require top, bottom, or both\"\n",
    "\n",
    "    quantizers = (real and [np.real] or []) + (imag and [np.imag] or [])\n",
    "    assert quantizers, \"require real, imag, or both\"\n",
    "    \n",
    "    def encode(response):\n",
    "        bit_parts = []\n",
    "        mask_parts = []\n",
    "        \n",
    "        for res_index in res_indexes:\n",
    "            for quantizer in quantizers:\n",
    "                res = response.iris_responses[res_index][::v_subsample, ::h_subsample]\n",
    "                bits = quantizer(res) > 0\n",
    "                mask = response.mask_responses[res_index][::v_subsample, ::h_subsample] >= mask_threshold\n",
    "\n",
    "                if mask_with_random:\n",
    "                    # Replace masked bits with random bits.\n",
    "                    fill_masked_with_random(bits, mask)\n",
    "                \n",
    "                if static_mask is not None:\n",
    "                    # Remove the bits not selected by the static mask.\n",
    "                    fill_masked_with_zeros(bits, static_mask[::v_subsample, ::h_subsample])\n",
    "                    # Treat non-selected bits as masked (False).\n",
    "                    mask &= static_mask[::v_subsample, ::h_subsample]\n",
    "                \n",
    "                bit_parts.append(bits)\n",
    "                mask_parts.append(mask)\n",
    "                assert mask.shape == bits.shape\n",
    "\n",
    "        return np.concatenate(bit_parts), np.concatenate(mask_parts)\n",
    "    \n",
    "    return encode\n",
    "\n",
    "def encode_pairs(pairs, encode_fn):\n",
    "    return [\n",
    "        (subject_id, encode_fn(response_a), encode_fn(response_b))\n",
    "        for subject_id, response_a, response_b in pairs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2986ed9-5e2c-4b63-b902-0c92523cddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances\n",
    "def masked_distance(x, x_mask, y, y_mask):\n",
    "    mask = x_mask & y_mask\n",
    "    hd = np.sum((x ^ y) & mask)\n",
    "    return hd / np.sum(mask)\n",
    "\n",
    "def masked_rotate(x, rotation):\n",
    "    return (\n",
    "        np.roll(x[0], rotation, axis=1),\n",
    "        np.roll(x[1], rotation, axis=1),\n",
    "    )\n",
    "\n",
    "def distance(x, y):\n",
    "    return masked_distance(x[0], x[1], y[0], y[1])\n",
    "\n",
    "def distance_raw(raw_x, raw_y):\n",
    "    return distance(encode_high(raw_x), encode_high(raw_y))\n",
    "\n",
    "def rotate_raw(raw_x, rotation):\n",
    "    iris_responses = [\n",
    "        np.roll(r, rotation, axis=1)\n",
    "        for r in raw_x.iris_responses\n",
    "    ]\n",
    "    mask_responses = [\n",
    "        np.roll(r, rotation, axis=1)\n",
    "        for r in raw_x.mask_responses\n",
    "    ]\n",
    "    return iris.IrisFilterResponse(iris_responses=iris_responses, mask_responses=mask_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5342022-6bd3-4cca-ad65-7c760ef584c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotations.\n",
    "def without_rotation(pairs, distance_fn, rotate_fn, max_rotation):\n",
    "    for subject_id, x, y in pairs:\n",
    "        distances = [\n",
    "            distance_fn(x, rotate_fn(y, rotation))\n",
    "            for rotation in range(-max_rotation, max_rotation+1)\n",
    "        ]\n",
    "        best_rotation = -max_rotation + np.argmin(distances)        \n",
    "        y_aligned = rotate_fn(y, best_rotation)\n",
    "        yield (subject_id, x, y_aligned)\n",
    "\n",
    "def remove_rotation(pairs, distance_fn=distance, rotate_fn=masked_rotate, max_rotation=15):\n",
    "    return list(without_rotation(pairs, distance_fn, rotate_fn, max_rotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e76d8fb-86ad-4405-8bb9-123be0c7dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boolean_iris(matrix, title=''):\n",
    "    plt.imshow(matrix, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02024c46-22e8-406a-bcc5-d25fa048fb6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282e0ee-578e-41a3-869f-0d433508c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 min\n",
    "encode_high = make_encoder()\n",
    "related_pairs = load_related_pairs(meta, count=None, unique_subjects=False)\n",
    "related_pairs_norot = remove_rotation(related_pairs, distance_fn=distance_raw, rotate_fn=rotate_raw)\n",
    "related_pairs_high = encode_pairs(related_pairs_norot, encode_high)\n",
    "shape_high = related_pairs_high[0][1][0].shape\n",
    "print(f\"Finished loading {len(related_pairs_high)} pairs,\", \"High-res\", shape_high, np.prod(shape_high), \"bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d5885-43cc-402c-b410-f115bb508490",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_pairs_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3c245-431e-41e5-9b12-30018dd2088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_array = np.array(related_pairs_high, dtype=object)\n",
    "subject_ids = np.repeat(tuples_array[:, 0], 2)  # Repeat each subject_id twice\n",
    "flattened_result = [item for tup in tuples_array for item in tup[1:]]\n",
    "iris_matrices, mask_matrices = zip(*flattened_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91deb9-4e4a-4f80-be97-b0386068e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_iris_df = pd.DataFrame({\n",
    "    'subject_id': subject_ids,\n",
    "    'iris_matrices': iris_matrices,\n",
    "    'mask_matrices': mask_matrices\n",
    "})\n",
    "true_iris_df['side'] = true_iris_df['subject_id'].apply(lambda x: x[-1])\n",
    "true_iris_df['subject_id'] = true_iris_df['subject_id'].apply(lambda x: x.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67897c5c-d9a4-40aa-b23f-cbd82deabf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "true_iris_df['iris_matrices_bytes'] = true_iris_df['iris_matrices'].apply(lambda matrix: matrix.tobytes())\n",
    "true_iris_df['mask_matrices_bytes'] = true_iris_df['mask_matrices'].apply(lambda matrix: matrix.tobytes())\n",
    "true_iris_df = (\n",
    "    true_iris_df\n",
    "    .drop_duplicates(subset=['subject_id', 'iris_matrices_bytes', 'mask_matrices_bytes'])\n",
    "    .drop(columns=['iris_matrices_bytes', 'mask_matrices_bytes'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(f'Final iris DataFrame contains {len(true_iris_df)} unique samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc16f2a-0925-4b06-a8af-68a5d27890b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Noise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8671c-86a2-4178-86d8-0b54ecd86b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_rotated_matrices(matrices, max_rotation):\n",
    "    return np.vstack([\n",
    "        np.roll(matrix, shift, axis=0).flatten()\n",
    "        for matrix, shift in product(matrices, range(-max_rotation, max_rotation + 1))\n",
    "    ])\n",
    "\n",
    "def get_pairwise_min_dist_across_rotations(iris_matrices, mask_matrices, max_rotation, lim_group_size=50):\n",
    "    if len(iris_matrices) > lim_group_size:\n",
    "        iris_matrices = iris_matrices.sample(lim_group_size)\n",
    "        mask_matrices = mask_matrices[iris_matrices.index]\n",
    "\n",
    "    # Create rotated matrices and masks\n",
    "    rotated_matrices = stack_rotated_matrices(iris_matrices, max_rotation)\n",
    "    rotated_masks = stack_rotated_matrices(mask_matrices, max_rotation)\n",
    "    \n",
    "    # Calculate pairwise Hamming distances considering only True values in the mask\n",
    "    valid_positions = np.expand_dims(rotated_masks, axis=1) & np.expand_dims(rotated_masks, axis=0)\n",
    "    differences = np.expand_dims(rotated_matrices, axis=1) != np.expand_dims(rotated_matrices, axis=0)\n",
    "    hamming_distances = np.sum(differences & valid_positions, axis=-1) / np.sum(valid_positions, axis=-1)\n",
    "    \n",
    "    # Mask self-comparisons with np.inf\n",
    "    matrix_indices = np.arange(len(iris_matrices)).repeat(2 * max_rotation + 1)\n",
    "    hamming_distances[matrix_indices[:, None] == matrix_indices[None, :]] = np.inf\n",
    "    \n",
    "    # Reshape and find minimum distances and indices\n",
    "    reshaped_distances = hamming_distances.reshape(len(iris_matrices), 2 * max_rotation + 1, len(iris_matrices), 2 * max_rotation + 1)\n",
    "    min_distances_per_matrix = np.min(reshaped_distances, axis=(1, 3))\n",
    "    min_indices = np.argmin(np.argmin(reshaped_distances, axis=1), axis=2)\n",
    "\n",
    "    # Extract only the lower triangle (excluding the diagonal)\n",
    "    lower_triangle_indices = np.tril_indices(len(iris_matrices), k=-1)\n",
    "    min_distances = min_distances_per_matrix[lower_triangle_indices]\n",
    "    optimal_rotations_list = np.array([np.unravel_index(min_indices[lower_triangle_indices], (2 * max_rotation + 1, 2 * max_rotation + 1))])\n",
    "    optimal_rotations_list = np.ravel(optimal_rotations_list, order='F')\n",
    "    \n",
    "    return min_distances, optimal_rotations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d351464d-aca1-4853-9850-da1b817bb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, MAX_ROT = [], 15\n",
    "for (subject_id, side), group in true_iris_df.groupby(['subject_id', 'side']):\n",
    "    results.append(get_pairwise_min_dist_across_rotations(group['iris_matrices'], group['mask_matrices'], max_rotation=MAX_ROT))\n",
    "distance_array, indices_array = zip(*results)\n",
    "nearest_pairwise_dist_w_rotations = np.concatenate(distance_array)\n",
    "nearest_pairwise_indices_w_rotations = np.concatenate(indices_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942209f1-d82a-4a0c-bcae-9bdc28d8805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,6))\n",
    "sns.histplot(nearest_pairwise_dist_w_rotations, stat='probability', bins=100, color='#BD2A2E')\n",
    "plt.title('Same Samples Distance Distribution (Noise)', fontsize=15, y=1.08)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6556cb6-ea26-4754-b26b-2359556bac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distribution(data, num_bins=100):\n",
    "    counts, bin_edges = np.histogram(data, bins=num_bins, density=True)\n",
    "    midpoints = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    probabilities = counts / np.sum(counts)\n",
    "    return midpoints, probabilities\n",
    "    \n",
    "def sample_from_distribution(midpoints, probabilities, sample_size=10):\n",
    "    return np.random.choice(midpoints, size=sample_size, p=probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e3c73-09bb-4c5f-bd08-8767402e83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "midpoints, probabilities = calculate_distribution(nearest_pairwise_dist_w_rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d364f8-9948-49d7-a947-88f3c2d25cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_distribution(midpoints, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d185d15-782b-43be-8f3e-ba937fd1adae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed('noise_distribution_bin_midpoints.npz', data=midpoints)\n",
    "# np.savez_compressed('noise_distribution_probability_distribution.npz', data=probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530917c7-3248-4cc5-b249-3364922724fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Short Mask Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c95c4-6f50-4d04-a4ba-fca9f2784776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_masks(mask_matrix):\n",
    "    # Adjust matrix to be built by 8 different masks\n",
    "    separated_matrix = mask_matrix.reshape(4, 16, 2, 100).transpose(2, 0, 1, 3)\n",
    "\n",
    "    # Calculate bottom row stats\n",
    "    inverted_last_rows = ~separated_matrix[:, :, -1, :]\n",
    "    mean_true_counts = inverted_last_rows.sum(axis=2).mean(axis=1) # Mean length\n",
    "    first_true_indices = np.where(\n",
    "        inverted_last_rows.any(axis=-1), np.argmax(inverted_last_rows, axis=-1), np.nan\n",
    "    )\n",
    "    last_true_indices = np.where(\n",
    "        inverted_last_rows.any(axis=-1), inverted_last_rows.shape[-1] - 1 - np.argmax(inverted_last_rows[:, :, ::-1], axis=-1), np.nan\n",
    "    )\n",
    "    mean_middle_indices = np.nanmean((first_true_indices + last_true_indices) / 2, axis=1) # Mean middle index\n",
    "\n",
    "    # Calculate longest column stats\n",
    "    true_counts = (~separated_matrix).sum(axis=2)\n",
    "    max_true_counts = np.max(true_counts, axis=2)\n",
    "    mean_max_true_counts = max_true_counts.mean(axis=1)\n",
    "    return (*mean_true_counts, *mean_middle_indices, *mean_max_true_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49284fb-773a-47cc-9e3d-a9ef5405ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mask_col_names = ['left_h_len', 'right_h_len', 'left_mid_ind', 'right_mid_ind', 'left_v_len', 'right_v_len']\n",
    "processed_mask_df = pd.DataFrame(\n",
    "    true_iris_df['mask_matrices'].apply(process_masks).tolist(), \n",
    "    columns=processed_mask_col_names, \n",
    "    index=true_iris_df.index\n",
    ")\n",
    "true_iris_df[processed_mask_col_names] = processed_mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101526c-6869-4804-8eb2-a47b3a93079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mask_df = pd.wide_to_long(\n",
    "    processed_mask_df.reset_index(), \n",
    "    stubnames=['left', 'right'], \n",
    "    i='index', \n",
    "    j='metric', \n",
    "    suffix='(h_len|mid_ind|v_len)', \n",
    "    sep='_'\n",
    ")\n",
    "processed_mask_df = (\n",
    "    pd.melt(processed_mask_df.reset_index(), id_vars=processed_mask_df.index.names, var_name='side')\n",
    "    .drop(columns='index')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dece600-a1b2-4f83-ad64-16a875216b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetgrid = sns.FacetGrid(processed_mask_df, col='metric', row='side', sharex=False, sharey=False, height=4, aspect=1.4)\n",
    "facetgrid.map_dataframe(sns.histplot, x='value', stat='probability', color='#019587', kde=True)\n",
    "[ax.grid(True) for ax in facetgrid.axes.flat]\n",
    "facetgrid.fig.suptitle(\"Iris masks derived distributions\", fontsize=20, y=1.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a89cdd-f6e6-414b-a7e1-0e09d65fa1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filtered = processed_mask_df[processed_mask_df['metric'] != 'mid_ind']\n",
    "zero_mask_perc = mask_filtered.groupby(['side']).apply(lambda group: (group['value'] == 0).mean())\n",
    "zero_mask_perc.rename('Percentage of no apparent mask').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469db0db-6251-42ee-a9e1-014855bc1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mask_df[processed_mask_df['value'] > 0 ].groupby(['side', 'metric']).agg({'mean', 'std'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79695af3-6b77-4772-9dd1-9455faa18aec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Synthethic Data Quality Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f560f9c-6af8-4d64-9aa5-002c5c8c60ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560aa55e-23d2-447f-b20f-9d8e0f2febe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79cd970-c318-42fa-b63b-3ca15e914608",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbfb315-abf1-481b-b080-c09104f6e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_iris_df = pd.concat([true_iris_df.assign(source='real'), synthetic_df], ignore_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b491e-b20b-4569-a77f-9821322c403b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Uniformity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc07b5-cd6c-4f83-9b5e-853c575e0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniformity_data = true_iris_df.drop_duplicates(subset=['subject_id', 'side'])\n",
    "# uniformity_data = np.stack(uniformity_data['iris_matrices'].values).reshape(312,12800)\n",
    "\n",
    "# hamming_distances = pdist(uniformity_data, metric='hamming')\n",
    "# hamming_distance_matrix = squareform(hamming_distances)\n",
    "\n",
    "# mds = MDS(n_components=249, dissimilarity='precomputed', random_state=1)\n",
    "# X_reduced = mds.fit_transform(hamming_distance_matrix)\n",
    "\n",
    "# X_reduced.min(axis=1), X_reduced.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250d520-2479-49f0-96cf-6f8799bda7b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Rotation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc08af-03df-4c45-85ee-8fa5cb971a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for rot in range(-MAX_ROT, MAX_ROT+1): \n",
    "    if rot == 0: # No rotation - distance is 0\n",
    "        continue\n",
    "\n",
    "    all_iris_df[f'low_{rot}'], all_iris_df[f'high_{rot}'] = zip(*all_iris_df['iris_matrices'].apply(\n",
    "        lambda matrix: [np.sum(part != np.roll(part, shift=rot, axis=1)) / part.size for part in np.split(matrix, 2, axis=0)]\n",
    "    ))\n",
    "\n",
    "    # Statistical Tests\n",
    "    for (source1, group1), (source2, group2) in combinations(all_iris_df.groupby('source'), 2):\n",
    "        for wavelet in ['low', 'high']:\n",
    "            group1_wavelet = group1[f'{wavelet}_{rot}']\n",
    "            group2_wavelet = group2[f'{wavelet}_{rot}']\n",
    "\n",
    "            ks_stat, ks_p_value = ks_2samp(group1_wavelet, group2_wavelet) # Kolmogorov-Smirnov test\n",
    "            t_stat, t_p_value = ttest_ind(group1_wavelet, group2_wavelet) # Student's t test\n",
    "            \n",
    "            # Store the results in a list (you can store anything you want here)\n",
    "            results.append({\n",
    "                'Rotation':rot,\n",
    "                'wavelet':wavelet,\n",
    "                'first_source':source1,\n",
    "                'second_source':source2,\n",
    "                'ks_stat':ks_stat,\n",
    "                't_stat':t_stat,\n",
    "                'ks_p_value':ks_p_value,\n",
    "                't_p_value':t_p_value,\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['passed_KS_test'] = results_df['ks_p_value'] >= alpha\n",
    "results_df['passed_t_test'] = results_df['t_p_value'] >= alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af089b-ae63-4f52-8c20-1145544a5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = (\n",
    "    results_df\n",
    "    .groupby(['first_source', 'second_source', 'wavelet'])[['ks_p_value', 'passed_KS_test', 't_p_value', 'passed_t_test']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "real_mask = results_df[['first_source', 'second_source']].isin(['real']).any(axis=1)\n",
    "results_df = results_df[real_mask]\n",
    "compared_method = np.where(results_df['first_source'] == 'real', results_df['second_source'], results_df['first_source'])\n",
    "results_df = (\n",
    "    results_df\n",
    "    .assign(compared_method=compared_method)\n",
    "    .drop(columns=['first_source', 'second_source'])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae00cc6-7a54-4da4-8534-97cf50d57962",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(['compared_method', 'wavelet']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e20ce9-71f9-4ae6-8c6b-177272c5a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.melt(\n",
    "    all_iris_df,\n",
    "    id_vars='source',\n",
    "    value_vars=[col for col in all_iris_df.columns if 'low_' in col or 'high_' in col],\n",
    "    var_name='wavelet_rotation',\n",
    "    value_name='Hamming Distance',\n",
    ")\n",
    "plot_df[['Wavelet', 'Rotation']] = plot_df['wavelet_rotation'].str.split('_', expand=True)\n",
    "plot_df.drop(columns=['wavelet_rotation'], inplace=True)\n",
    "plot_df['Rotation'] = plot_df['Rotation'].astype(int)\n",
    "plot_df['Source'] = plot_df['source'] + ', ' + plot_df['Wavelet'] + ' wavelet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98f3f9-28be-4b0c-9fc8-e458ee7eac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_linear_analysis = (\n",
    "    plot_df\n",
    "    .groupby(['Rotation', 'Wavelet', 'source'])['Hamming Distance']\n",
    "    .agg({'mean', 'std'})\n",
    "    .reset_index()\n",
    ").rename(columns={'source':'Source'})\n",
    "dist_linear_analysis = pd.melt(\n",
    "    dist_linear_analysis,\n",
    "    id_vars=['Rotation', 'Wavelet', 'Source'],\n",
    "    value_vars=['std', 'mean'],\n",
    "    var_name='Metric'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1da68-c36c-443d-a918-3032c1a31806",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = dist_linear_analysis['Source'] == 'real'\n",
    "facetgrid = sns.FacetGrid(dist_linear_analysis[mask], col='Metric', height=5, aspect=2, sharex=False, sharey=False)\n",
    "facetgrid.map_dataframe(sns.lineplot, x='Rotation', y='value', hue='Wavelet', palette='husl')\n",
    "[(ax.grid(True), ax.legend()) for ax in facetgrid.axes.flat]\n",
    "facetgrid.fig.suptitle(f\"Mean and Std of real iris samples, in relation to rotation\", fontsize=15, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856dd14e-590f-424f-acab-ea3f846fe3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in ['gaussian', 'voter', 'voter_bryan', 'voter_gaussian']:\n",
    "    mask = dist_linear_analysis['Source'].isin(['real']+[source])\n",
    "    facetgrid = sns.FacetGrid(dist_linear_analysis[mask], col='Metric', height=5, aspect=2, sharex=False, sharey=False)\n",
    "    facetgrid.map_dataframe(sns.lineplot, x='Rotation', y='value', hue='Wavelet', palette='husl', style='Source')\n",
    "    [(ax.grid(True), ax.legend()) for ax in facetgrid.axes.flat]\n",
    "    facetgrid.fig.suptitle(f\"Mean and Std of real and {source.replace('_', ' ')} iris samples, in relation to rotation\", fontsize=15, y=1.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add84832-fc0a-40ff-bb36-749f1ca52483",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in ['gaussian', 'voter', 'voter_gaussian', 'voter_bryan']:\n",
    "    mask = plot_df['source'].isin(['real']+[source])\n",
    "    facetgrid = sns.FacetGrid(plot_df[mask], col='Rotation', hue='Source', palette='husl', col_wrap=5, sharex=False)\n",
    "    facetgrid.map_dataframe(sns.histplot, x='Hamming Distance', stat='probability')\n",
    "    [ax.grid(True) for ax in facetgrid.axes.flat]\n",
    "    facetgrid.add_legend()\n",
    "    facetgrid.fig.suptitle(f\"Distance distribution upon rotation\\nReal irises to {source.replace('_', ' ')} distributions\", fontsize=20, y=1.03)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747151a5-b479-478d-91e2-1d4bacd3daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_iris_samples_df = all_iris_df[all_iris_df['source'] == 'real'].drop_duplicates(subset=['subject_id', 'side'])\n",
    "real_iris_samples_df['mean_bw_ratio_low'], real_iris_samples_df['mean_bw_ratio_high'] = zip(*real_iris_samples_df['iris_matrices'].apply(\n",
    "    lambda matrix: [part.mean(axis=0) for part in np.split(matrix, 2, axis=0)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a79015-a1ed-457b-ae9d-c4da6d00c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "for wavelength in ['low', 'high']:\n",
    "    bw_ratio_matrix = np.column_stack(real_iris_samples_df[f'mean_bw_ratio_{wavelength}'].values)\n",
    "    mean_values = bw_ratio_matrix.mean(axis=1)\n",
    "    ci = 1.96 * bw_ratio_matrix.std(axis=1) / np.sqrt(bw_ratio_matrix.shape[1])  # 95% CI\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=range(bw_ratio_matrix.shape[0]), y=mean_values, errorbar=None, label='Mean True / False Ratio')\n",
    "    plt.fill_between(range(bw_ratio_matrix.shape[0]), mean_values - ci, mean_values + ci, color='b', alpha=0.3, label='95% CI')\n",
    "    plt.title(f\"Mean True / False Ratio, {wavelength} wavelength with 95% Confidence Interval\")\n",
    "    plt.xlabel(\"Iris matrix x-axis\")\n",
    "    plt.ylabel(\"True / False Ratio\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b54241e-3962-48c6-bc56-b80fedac1af3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Boolean Ratio Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fe8d4-3d98-4dfd-b3e8-e6e732b4bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_iris_df['Boolean Ratio'] = all_iris_df['iris_matrices'].apply(lambda matrix: matrix.mean())\n",
    "stats_df = all_iris_df.groupby('source')['Boolean Ratio'].agg({'mean', 'std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251e31c-a828-42b6-85b5-622e3b5f4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetgrid = sns.FacetGrid(all_iris_df, col='source', sharex=False, sharey=False)\n",
    "facetgrid.map_dataframe(sns.histplot, x='Boolean Ratio', stat='probability', kde=True, color='#FF7A48')\n",
    "title_template = \"Source: {col_name}\\nMean: {mean:.2f}, Std: {std:.2f}\"\n",
    "facetgrid.set_titles(col_template=\"{col_name}\")\n",
    "for ax, col_value in zip(facetgrid.axes.flat, facetgrid.col_names):\n",
    "    mean = stats_df.loc[col_value, 'mean']\n",
    "    std = stats_df.loc[col_value, 'std']\n",
    "    ax.set_title(title_template.format(col_name=col_value, mean=mean, std=std))\n",
    "    ax.grid(True)\n",
    "facetgrid.fig.suptitle(f\"True / False Ratio Validation\", fontsize=20, y=1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a98c5-c0e7-4df7-a77d-1bda025d97d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Nearest to Random Dist Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ff6d5-f674-42d4-8901-a125c0753377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_rotated_matrices(matrices, max_rotation):\n",
    "    return np.vstack([\n",
    "        np.roll(matrix, shift, axis=0).flatten()\n",
    "        for matrix in matrices\n",
    "        for shift in range(-max_rotation, max_rotation + 1)\n",
    "    ])\n",
    "\n",
    "def get_min_and_random_dist_across_rotations(iris_matrices, mask_matrices, max_rotation):\n",
    "    num_matrices = len(iris_matrices)\n",
    "    num_rotations = 2 * max_rotation + 1\n",
    "\n",
    "    # Rotate matrices and masks, reshape to (num_matrices, num_rotations, flattened_size)\n",
    "    rotated_matrices = stack_rotated_matrices(iris_matrices, max_rotation).reshape(num_matrices, num_rotations, -1)\n",
    "    rotated_masks = stack_rotated_matrices(mask_matrices, max_rotation).reshape(num_matrices, num_rotations, -1)\n",
    "\n",
    "    closest_distances, random_distances = [], []\n",
    "    for i in range(num_matrices):\n",
    "        # Current matrix rotations and masks\n",
    "        current_rotated_matrix = rotated_matrices[i]\n",
    "        current_rotated_mask = rotated_masks[i]\n",
    "\n",
    "        # Extract other matrices' rotations excluding the current\n",
    "        other_rotated_matrices = np.delete(rotated_matrices, i, axis=0).reshape(-1, rotated_matrices.shape[-1])\n",
    "        other_rotated_masks = np.delete(rotated_masks, i, axis=0).reshape(-1, rotated_masks.shape[-1])\n",
    "\n",
    "        # Calculate valid positions and Hamming distances\n",
    "        valid_positions = current_rotated_mask[:, None] & other_rotated_masks\n",
    "        differences = current_rotated_matrix[:, None] != other_rotated_matrices\n",
    "\n",
    "        # Calculate Hamming distances\n",
    "        hamming_distances = np.sum(differences & valid_positions, axis=-1) / np.sum(valid_positions, axis=-1)\n",
    "        \n",
    "        # Find the minimum distance and a random distance\n",
    "        closest_distances.append(np.min(hamming_distances))\n",
    "        random_distances.append(np.random.choice(hamming_distances.flatten()))\n",
    "\n",
    "    return pd.DataFrame({\"closest_dist\": closest_distances, \"random_dist\": random_distances}, index=iris_matrices.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b2ae0-0dbd-482a-b938-004ab6a3dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_iris_samples_df = all_iris_df[all_iris_df['source'] == 'real'].drop_duplicates(subset=['subject_id', 'side'])\n",
    "balanced_non_real_samples = (\n",
    "    all_iris_df[all_iris_df['source'] != 'real']\n",
    "    .groupby('source')\n",
    "    .apply(lambda group: group.sample(len(real_iris_samples_df)))\n",
    ")\n",
    "sub_iris_df = pd.concat([real_iris_samples_df, balanced_non_real_samples], ignore_index=True)\n",
    "sub_iris_df.groupby('source').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6012a11e-b2a2-4a07-816d-d27be40e6414",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for source, group in sub_iris_df.groupby('source'):\n",
    "    results.append(get_min_and_random_dist_across_rotations(group['iris_matrices'], group['mask_matrices'], max_rotation=MAX_ROT).assign(source = source))\n",
    "results = pd.concat(results)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cb21b-c398-4731-9272-9c3d59ec3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.melt(\n",
    "    results.rename(columns={'source':'Source'}), \n",
    "    id_vars='Source', \n",
    "    value_vars=['random_dist', 'closest_dist'],\n",
    "    var_name='Distance From',\n",
    "    value_name='Hamming Distance'\n",
    ")\n",
    "plot_df['Distance From'] = plot_df['Distance From'].apply(lambda x: x.split('_')[0].capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcfdde2-a382-43c6-81e1-cf16c8123b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetgrid = sns.FacetGrid(plot_df, col='Distance From', hue='Source', palette='husl', height=4, aspect=2, sharex=False, sharey=False)\n",
    "facetgrid.map_dataframe(sns.histplot, x='Hamming Distance', stat='probability', kde=True)\n",
    "[ax.grid(True) for ax in facetgrid.axes.flat]\n",
    "facetgrid.add_legend()\n",
    "facetgrid.fig.suptitle(f\"Distance from Random / Nearest iris, by data source\", fontsize=20, y=1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32222ce9-4477-4738-8374-54dade3d2ff0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Comparing pair-wise distance distributions (to Daugman survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb42c5b-e45f-4eb8-9e06-f1299356d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_real_iris_samples = all_iris_df[all_iris_df['source'] != 'real']\n",
    "real_iris_samples_df = all_iris_df[all_iris_df['source'] == 'real'].drop_duplicates(subset=['subject_id', 'side'])\n",
    "sub_iris_df = pd.concat([non_real_iris_samples, real_iris_samples_df], ignore_index=True)\n",
    "sub_iris_df.groupby('source').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbf490-d8b7-4fa1-86c1-a0b9461ffc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_dist_vector(iris_matrices):\n",
    "    reshaped_matrices = np.stack(iris_matrices.values).reshape(len(iris_matrices),-1)\n",
    "    neq_matrices = np.expand_dims(reshaped_matrices, axis=1) != np.expand_dims(reshaped_matrices, axis=0)\n",
    "    dist_vector = (np.sum(neq_matrices, axis=2) / reshaped_matrices.shape[1]).flatten()\n",
    "    return dist_vector[dist_vector > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb0014-b4de-417d-b1dc-e335e74fa566",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_series = sub_iris_df.groupby('source').apply(lambda group: get_pairwise_dist_vector(group['iris_matrices']))\n",
    "sources = np.repeat(distances_series.index.values, distances_series.str.len())\n",
    "distances = np.concatenate(distances_series.values)\n",
    "plot_df = pd.DataFrame({'Source': sources, 'Distances': distances})\n",
    "plot_df = (\n",
    "    plot_df\n",
    "    .groupby('Source', group_keys=False)\n",
    "    .apply(lambda group: group.sample((plot_df['Source'] == 'real').sum()))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262fb85-fede-41ad-bf10-4e7c23ea2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(plot_df, x='Distances', stat='probability', hue='Source', palette='husl', kde=True)\n",
    "plt.grid(True)\n",
    "plt.title('Pairwise Distance Distribution', fontsize=15, y=1.07)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377dc8c-385e-49f3-894f-6341fa08f151",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = plot_df.groupby('Source')['Distances'].agg({'mean', 'std'})\n",
    "stats_df['N'] = (stats_df['mean'] * (1 - stats_df['mean'])) / stats_df['std']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb54d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetgrid = sns.FacetGrid(plot_df, col='Source', sharex=False, sharey=False)\n",
    "facetgrid.map_dataframe(sns.histplot, x='Distances', stat='probability', kde=True, color='#FF7A48')\n",
    "title_template = \"Source: {col_name}\\nMean: {mean:.2f}, Std: {std:.2f}, N: {N:.0f}\"\n",
    "facetgrid.set_titles(col_template=\"{col_name}\")\n",
    "for ax, col_value in zip(facetgrid.axes.flat, facetgrid.col_names):\n",
    "    mean, std, N = stats_df.loc[col_value, ['mean', 'std', 'N']] \n",
    "    ax.set_title(title_template.format(col_name=col_value, mean=mean, std=std, N=N))\n",
    "    ax.grid(True)\n",
    "facetgrid.fig.suptitle(f\"Pairwise Distance Distribution\\nStats from Daugman survey - Mean: 0.499, Std: 0.0317, N=249\", fontsize=20, y=1.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a268ea4-366d-4f82-b999-91c0607862b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vertical / Horizontal local behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c95d42-05af-4c51-b78a-092731b23d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44544bca-9a92-4e34-8972-e191f2fcbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_iris_df[f'low_vchanges'], all_iris_df[f'high_vchanges'] = zip(*all_iris_df['iris_matrices'].apply(\n",
    "    lambda matrix: [np.sum(np.diff(part, axis=0) != 0, axis=0) for part in np.split(matrix, 2, axis=0)]\n",
    "))\n",
    "all_iris_df[f'low_hchanges'], all_iris_df[f'high_hchanges'] = zip(*all_iris_df['iris_matrices'].apply(\n",
    "    lambda matrix: [np.sum(np.diff(part, axis=1) != 0, axis=1) for part in np.split(matrix, 2, axis=0)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb8401-f00a-4e53-a13c-b497c0b4453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics(mean, std):\n",
    "    processed_lst = [\n",
    "        pd.DataFrame(index=data.index, data=data.tolist()).reset_index() for data in [mean, std]\n",
    "    ]\n",
    "    processed_lst = [\n",
    "        pd.melt(data, id_vars='source', value_name=metric, var_name='axis_position') for metric, data in zip(['mean', 'std'], processed_lst)\n",
    "    ]\n",
    "    result = processed_lst[0].join(processed_lst[1].set_index(['source', 'axis_position']), on=['source', 'axis_position'], how='left')\n",
    "    result['axis_position'] = result['axis_position'].astype(int)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f62f4c-2861-4bea-a1ac-1241c787103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data(wavelet, orientation):\n",
    "    col_name = f'{wavelet}_vchanges' if orientation == 'vertical' else f'{wavelet}_hchanges'\n",
    "    mean_values = all_iris_df.groupby('source').apply(lambda group: np.vstack(group[col_name]).mean(axis=0))\n",
    "    std_values = all_iris_df.groupby('source').apply(lambda group: np.vstack(group[col_name]).std(axis=0))\n",
    "    return process_metrics(mean_values, std_values).assign(wavelet = wavelet, orientation = orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06002c-4fcd-49c4-80e6-84cbf475dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst = [get_processed_data(wavelet, orientation) for wavelet, orientation in product(['low', 'high'], ['vertical', 'horizontal'])]\n",
    "data = pd.concat(data_lst)\n",
    "data['segment'] = data['wavelet'] + ' wavelets, ' + data['orientation'] + ' behavior'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564c9b9-deec-4200-950f-713448970405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_ci(data, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    ci_upper = data['mean'] + 1.96 * data['std']\n",
    "    ci_lower = data['mean'] - 1.96 * data['std']\n",
    "    if data['orientation'].iloc[0] == 'vertical':\n",
    "        plt.plot(data['axis_position'], data['mean'], lw=3, color='#4CABA6')\n",
    "        plt.fill_between(data['axis_position'], ci_lower, ci_upper, color='#F2CDAC', alpha=0.2)\n",
    "        ax.set_ylabel('mean cluster change')\n",
    "        ax.set_xlabel('horizontal position')\n",
    "    else:\n",
    "        plt.plot(data['mean'], data['axis_position'], lw=3, color='#4CABA6')\n",
    "        plt.fill_betweenx(data['axis_position'], ci_lower, ci_upper, color='#F2CDAC', alpha=0.2)\n",
    "        ax.set_ylabel('vertical position')\n",
    "        ax.set_xlabel('mean cluster change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86099314-f814-48de-aaf5-48893acfe560",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetgrid = sns.FacetGrid(data, col='source', row='segment', height=4, aspect=1.4, sharey=False, sharex=False)\n",
    "facetgrid.map_dataframe(plot_with_ci)\n",
    "facetgrid.fig.suptitle('Mean cluster change with 95% Confidence Interval Across Sources', fontsize=20, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bbe46-5a03-4239-b9da-a68684d9f900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
