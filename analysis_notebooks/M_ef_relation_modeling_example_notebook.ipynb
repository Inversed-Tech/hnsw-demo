{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765202e0-2135-4dce-a4d2-fc0d3fe39f0b",
   "metadata": {},
   "source": [
    "# Imports and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37203805-61d2-4963-bd9e-dd32f226ef24",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab25026-3e7e-4abb-bddb-7f95caf6e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import time\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3b2fc7-11b3-41b6-b5d4-83de26cffab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-Party Library Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from functools import reduce\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37df96d-fb70-494a-8c5d-c9467a597bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Library Imports\n",
    "from iris.io.dataclasses import IrisTemplate\n",
    "from iris_integration import (\n",
    "    iris_with_noise,\n",
    "    irisint_make_query as make_query,\n",
    "    irisint_query_to_vector as query_to_vector,\n",
    "    irisint_distance as distance,\n",
    ")\n",
    "import hnsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712da865-498e-4919-b677-820af870ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "n_jobs = 6  # Number of parallel jobs, adjust to running CPU\n",
    "DIM = (2, 32, 200)  # Iris dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a87c9-389d-4af7-8e91-e73678ed0146",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58be7b1f-eeb4-4b8f-b9a7-5a7b12e4d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_update_time = time.time() # Tracks the last update time for progress messages\n",
    "def print_progress(msg, delay=1, force_print=False):\n",
    "    \"\"\"Prints a progress message, updating periodically or immediately if forced.\"\"\"\n",
    "    global last_update_time\n",
    "    if (time.time() - last_update_time > delay) or force_print:\n",
    "        sys.stdout.write('\\r' + ' ' * 200)  # Clear previous message\n",
    "        sys.stdout.write(f\"\\r{msg}\")\n",
    "        sys.stdout.flush()\n",
    "        last_update_time = time.time()  # Update the last printed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eeecbd8-35ea-4ddd-a7c5-986de117ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, filename):\n",
    "    \"\"\"Saves a Python object to a file using pickle.\"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(obj, file)  # Save the object\n",
    "    print(f\"Object successfully saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79bf8226-429d-458c-b8f0-02d661800929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    \"\"\"Loads a Python object from a pickle file.\"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        obj = pickle.load(file)  # Load the object\n",
    "    print(f\"Object successfully loaded from {filename}\")\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d440eb95-9f29-4541-9788-bab19727510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boolean_iris(matrix, title=''):\n",
    "    \"\"\"Plots a grayscale image of a 2D boolean matrix.\"\"\"\n",
    "    plt.imshow(matrix, cmap='gray')  # Render the matrix as an image\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19023710-44cf-4c39-ad7b-0f294af98015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_scaled_string(n):\n",
    "    \"\"\"Converts a large integer into a human-readable format (e.g., 1.2K, 3M).\"\"\"\n",
    "    suffixes = ['', 'K', 'M', 'B', 'T']\n",
    "    idx = max(0, min(len(suffixes) - 1, int((len(str(abs(n))) - 1) / 3)))\n",
    "    scaled = n / (1000 ** idx)\n",
    "    # Format scaled number with suffix\n",
    "    return f\"{scaled:.1f}{suffixes[idx]}\" if scaled % 1 else f\"{int(scaled)}{suffixes[idx]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4635270-2229-42e2-ba1f-46cdec40efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_avg_time(current_avg, denominator, new_time):\n",
    "    \"\"\"Updates a running average given a new value.\"\"\"\n",
    "    # Weighted average calculation\n",
    "    return ((current_avg * denominator) + new_time) / (denominator + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a162f8ea-4648-4370-92fd-1f3478f6138f",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dbb8627-0ad3-4dba-82cc-3fba8ef260cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_partial_file(filename, num_bits):\n",
    "    \"\"\"Reads a specified number of bits from a binary file.\"\"\"\n",
    "    num_bytes = (num_bits + 7) // 8  # Calculate the number of bytes needed\n",
    "    with open(filename, 'rb') as f:\n",
    "        chunk = f.read(num_bytes)  # Read the required bytes\n",
    "    return np.frombuffer(chunk, dtype=np.uint8)  # Convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd77621-f0a1-4496-bc85-2ca7b79aa17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_reshape_masks(filename, num_masks, DIM=DIM):\n",
    "    \"\"\"Loads and reshapes binary masks, duplicating them to fit the target dimensions.\"\"\"\n",
    "    # Read and unpack binary data into boolean array\n",
    "    flattened_data = np.unpackbits(read_partial_file(filename, ((DIM[1] // 2) * DIM[2]) * num_masks))\n",
    "    boolean_arrays = flattened_data.reshape((num_masks, DIM[1] // 2, DIM[2]))  # Reshape to mask dimensions\n",
    "    vertically_stacked = np.tile(boolean_arrays, (1, 2, 1))  # Duplicate vertically to restore full height\n",
    "    duplicated_arrays = np.repeat(vertically_stacked[:, np.newaxis, :, :], DIM[0], axis=1)  # Duplicate across depth\n",
    "    return duplicated_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc01eb36-eb54-4e21-8a77-8608a7726ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_reshape_irises(path_low, path_high, num_samples, DIM=DIM):\n",
    "    \"\"\"Loads and reshapes low and high-resolution iris data into a boolean array.\"\"\"\n",
    "    low_high_lst = [\n",
    "        np.unpackbits(\n",
    "            read_partial_file(path, (reduce(mul, DIM[1:]) * num_samples)), bitorder=\"little\"\n",
    "        ).reshape(num_samples, *DIM[1:]) for path in [path_low, path_high]\n",
    "    ]\n",
    "    return np.concatenate(low_high_lst, axis=1).astype(bool)  # Combine and cast to boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7759adc-0a87-4986-befa-de555ae2ea55",
   "metadata": {},
   "source": [
    "## Test Functions and DB Buildup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d100dd-eb96-479f-9f72-5bdd2db031e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(db, iris_df, db_size, force_layer=None):\n",
    "    \"\"\"Updates the database by inserting new iris templates up to the target size.\"\"\"\n",
    "    db_current_size = db.get_stats()['db_size']  # Current size of the database\n",
    "    if (db_size - db_current_size) <= 0:\n",
    "        return  # Exit if no additional entries are needed\n",
    "    \n",
    "    # Select new iris templates to insert\n",
    "    new_irises = iris_df.loc[range(db_current_size, db_size), 'Template']\n",
    "    for i, iris in enumerate(new_irises):\n",
    "        print_progress(\n",
    "            f'Currently building {int_to_scaled_string(db_size)} DB, with M={db.M}, efConstruction={db.efConstruction}. '\n",
    "            f'Insertion Progress: {(i+1)/len(new_irises):.1%}'\n",
    "        )\n",
    "        db.insert(make_query(iris), insert_layer=force_layer)  # Insert into the database\n",
    "    iris_df.loc[range(db_current_size, db_size), 'Inserted'] = True  # Mark inserted templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8fcaa23-b434-4eb2-856e-bca6f7d41432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_array_to_iris_df(iris_array, mask_array):\n",
    "    \"\"\"Converts numpy arrays of iris and mask data into a DataFrame with iris templates.\"\"\"\n",
    "    def create_iris_template(matrix, mask):\n",
    "        return IrisTemplate(\n",
    "            iris_codes=matrix,\n",
    "            mask_codes=mask\n",
    "            iris_code_version=\"v3.0\" # compatibility issues in version open-iris==1.0.0\n",
    "        )\n",
    "    # Generate iris templates in parallel\n",
    "    iris_templates = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(create_iris_template)(list(iris), list(mask)) for iris, mask in zip(iris_array, mask_array)\n",
    "    )\n",
    "    return pd.DataFrame({'Template': iris_templates}).assign(Inserted=False)  # Add a column for insertion status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d199413c-036a-43f1-abf8-ec46cb71c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(db, idx, iris, noise, efSearch, K):\n",
    "    \"\"\"Runs a single experiment by searching for a noisy iris query in the database.\"\"\"\n",
    "    noisy_query = make_query(iris_with_noise(iris, noise_level=noise))  # Create noisy query\n",
    "    res = db.search(noisy_query, K, ef=efSearch)  # Perform the search\n",
    "    return any(idx == tup[1] for tup in res)  # Check if any result matches the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff19d2-cd62-4dcb-9111-ebc31548d524",
   "metadata": {},
   "source": [
    "# Stats Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35011c-4e0c-4442-8002-d495e6fbb689",
   "metadata": {},
   "source": [
    "## Decision Boundary Stability Over DB Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64f164da-8b3b-4c63-b6ef-8e7cf1fb6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Configuration\n",
    "efConstruction = 128  # I was using 256, maybe run it with 128? \n",
    "db_size_range = np.arange(50000, 250001, 5000)  # Range of database sizes\n",
    "M_range = np.arange(16, 193, 8)  # Range of `M` values for HNSW\n",
    "efSearch_range = np.arange(16, 193, 8)  # Range of `efSearch` values for HNSW\n",
    "K = 1  # Number of nearest neighbors to retrieve\n",
    "\n",
    "# Experiment Settings\n",
    "num_experiments = 1000  # Number of experiments to run\n",
    "noise_level = 0.3  # Level of noise to apply during experiments\n",
    "\n",
    "# Results Output\n",
    "results_path = f'analysis_data/efConstruction{efConstruction}_db_size_stability_results.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930796a6-6d2f-4fef-984c-79cf1daff3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define synthetic data size and file paths (modify these paths as needed)\n",
    "synthetic_data_size = 2**22  # Size of synthetic data \n",
    "path_masks = f'path/to/synthetic_data/{int_to_scaled_string(synthetic_data_size)}_mask_arrays.dat'  # Modify this path\n",
    "path_iris_low = 'path/to/synthetic_data/2_23_voter_arrays_90k_b090.dat'  # Modify this path\n",
    "path_iris_high = 'path/to/synthetic_data/2_23_voter_arrays_14k_b010.dat'  # Modify this path\n",
    "\n",
    "# Load and reshape data\n",
    "loaded_masks = load_and_reshape_masks(path_masks, db_size_range.max()).astype(bool)  # Load and convert to boolean\n",
    "loaded_irises = load_and_reshape_irises(path_iris_low, path_iris_high, db_size_range.max())\n",
    "\n",
    "# Create a DataFrame from the loaded iris and mask data\n",
    "iris_df = numpy_array_to_iris_df(\n",
    "    loaded_irises.reshape(db_size_range.max(), *DIM),\n",
    "    loaded_masks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5dbe9-baf3-4e7b-bacb-1e15a7baf40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for results and average iteration time\n",
    "results_lst, avg_iteration_time = [], 0\n",
    "\n",
    "# Iterate over M values in M_range\n",
    "for j, M in enumerate(M_range):\n",
    "    start_time = time.time()  # Track the start time for this iteration\n",
    "    \n",
    "    # Reset 'Inserted' status in iris DataFrame\n",
    "    iris_df['Inserted'] = False\n",
    "\n",
    "    # Initialize the HNSW database with current M value\n",
    "    db = hnsw.HNSW(\n",
    "        M=M,\n",
    "        efConstruction=efConstruction,\n",
    "        m_L=1 / np.log(M),  # Layer multiplier\n",
    "        distance_func=distance,\n",
    "        query_to_vector_func=query_to_vector\n",
    "    )\n",
    "\n",
    "    # Iterate over database sizes in db_size_range\n",
    "    for db_size in db_size_range:\n",
    "        update_db(db, iris_df, db_size)  # Update the database with current size\n",
    "\n",
    "        # Iterate over efSearch values in efSearch_range\n",
    "        for efSearch in efSearch_range:\n",
    "            # Sample data for experiments\n",
    "            indices, irises = iris_df.loc[iris_df['Inserted'], 'Template'].sample(\n",
    "                num_experiments\n",
    "            ).reset_index().T.values\n",
    "            \n",
    "            # Run experiments in parallel\n",
    "            with parallel_backend('threading'):\n",
    "                results = Parallel(n_jobs=n_jobs)(\n",
    "                    delayed(run_single_experiment)(\n",
    "                        db, indices[i], irises[i], noise_level, efSearch, K\n",
    "                    ) for i in range(num_experiments)\n",
    "                )\n",
    "\n",
    "            # Store results for this configuration\n",
    "            results_lst.append((\n",
    "                M, efConstruction, db_size, efSearch, \n",
    "                np.mean(results), len(db.layers[1])\n",
    "            ))\n",
    "        \n",
    "        # Save intermediate results to a parquet file\n",
    "        results_df = pd.DataFrame(\n",
    "            results_lst, \n",
    "            columns=['M', 'efConstruction', 'DB_Size', 'efSearch', 'Recall', 'Layer 1 Size']\n",
    "        )\n",
    "        results_df.to_parquet(results_path)\n",
    "\n",
    "    # Calculate and update average iteration time\n",
    "    end_time = time.time() - start_time\n",
    "    avg_iteration_time = update_avg_time(avg_iteration_time, j, end_time)\n",
    "\n",
    "    # Print progress and estimated time remaining\n",
    "    print_progress(\n",
    "        f'\\r{(j+1)/len(M_range):.1%} Completed, '\n",
    "        f'Average Iteration Time: {avg_iteration_time/3600:.1f} hours, '\n",
    "        f'EOC: {(avg_iteration_time/3600)*(len(M_range)-(j+1)):.1f} hours\\n', force_print=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9206f-f011-4e42-8522-a42b5ed4e62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
